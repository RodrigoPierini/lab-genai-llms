{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB GenAI - LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI GPT API Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't have a premium account on OpenAI, so for this lab, you can find the codes without the API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a simple chatbot that can answer basic questions about a given topic (e.g., history, technology).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "#openai.api_key  = os.getenv('your-api-key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key = 'your-api-key'\n",
    "\n",
    "def generate_response(prompt, temperature=0.7, max_tokens=100, top_p=0.9, frequency_penalty=0, presence_penalty=0, n=1, stop=None):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        n=n,\n",
    "        stop=stop\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"What are the key differences between 4G and 5G technology?\"\n",
    "\n",
    "# Generate a response with the default parameters\n",
    "response = generate_response(prompt)\n",
    "\n",
    "print(\"Response:\", response)\n",
    "\n",
    "# Experiment with different parameters\n",
    "response_deterministic = generate_response(prompt, temperature=0.2)\n",
    "print(\"\\nDeterministic Response:\", response_deterministic)\n",
    "\n",
    "response_creative = generate_response(prompt, temperature=1.0, max_tokens=150, top_p=1.0, frequency_penalty=0.5, presence_penalty=0.5)\n",
    "print(\"\\nCreative and Diverse Response:\", response_creative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Write a script that takes a long text input and summarizes it into a few sentences.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'your-api-key'\n",
    "\n",
    "def summarize_text(text, temperature=0.7, max_tokens=100, top_p=0.9, frequency_penalty=0, presence_penalty=0, best_of=1, logprobs=None):\n",
    "    \"\"\"\n",
    "    Summarizes the given text into a shorter version.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The long text input to be summarized.\n",
    "        temperature (float): The temperature for the model (controls randomness).\n",
    "        max_tokens (int): The maximum number of tokens for the summary.\n",
    "        top_p (float): The top_p parameter for nucleus sampling (controls diversity).\n",
    "        frequency_penalty (float): The frequency penalty to reduce repetition.\n",
    "        presence_penalty (float): The presence penalty to encourage new topics.\n",
    "        best_of (int): Generates multiple completions server-side and returns the best one.\n",
    "        logprobs (int): Returns the log probabilities of the top logprobs tokens.\n",
    "\n",
    "    Returns:\n",
    "        str: The summarized text.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-3.5-turbo\",  # Use the appropriate model name\n",
    "        prompt=f\"Summarize the following text:\\n\\n{text}\\n\\nSummary:\",\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        best_of=best_of,\n",
    "        logprobs=logprobs,\n",
    "        stop=[\"\\n\"]\n",
    "    )\n",
    "    summary = response.choices[0].text.strip()\n",
    "    return summary, response if logprobs is not None else summary\n",
    "\n",
    "# Define the long text input\n",
    "long_text = \"\"\"\n",
    "The rise of artificial intelligence has revolutionized numerous industries by enabling machines to perform tasks that typically require human intelligence. AI applications range from autonomous vehicles and virtual assistants to advanced data analytics and personalized recommendations. The integration of AI technologies has not only improved efficiency and productivity but also opened new avenues for innovation and economic growth. However, the rapid development of AI also raises ethical and societal concerns, such as job displacement, privacy issues, and the potential for biased decision-making. Addressing these challenges requires a collaborative effort among policymakers, industry leaders, and researchers to ensure that AI technologies are developed and deployed responsibly, maximizing their benefits while minimizing their risks.\n",
    "\"\"\"\n",
    "\n",
    "# Generate a summary of the long text with default parameters\n",
    "summary, response = summarize_text(long_text, logprobs=5)\n",
    "\n",
    "print(\"Summary:\", summary)\n",
    "\n",
    "if response:\n",
    "    print(\"\\nLogprobs:\", response.choices[0].logprobs)\n",
    "\n",
    "# Experiment with different parameters\n",
    "summary_deterministic = summarize_text(long_text, temperature=0.2)[0]\n",
    "print(\"\\nDeterministic Summary:\", summary_deterministic)\n",
    "\n",
    "summary_creative = summarize_text(long_text, temperature=1.0, max_tokens=150, top_p=1.0, frequency_penalty=0.5, presence_penalty=0.5, best_of=3)[0]\n",
    "print(\"\\nCreative and Diverse Summary:\", summary_creative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Develop a tool that translates text from one language to another using the API.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'your-api-key'\n",
    "\n",
    "def translate_text(text, source_lang=\"English\", target_lang=\"Spanish\", temperature=0.7, max_tokens=100, top_p=0.9, frequency_penalty=0, presence_penalty=0, echo=False, logit_bias=None):\n",
    "    \"\"\"\n",
    "    Translates the given text from source language to target language.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to be translated.\n",
    "        source_lang (str): The source language of the text.\n",
    "        target_lang (str): The target language to translate the text into.\n",
    "        temperature (float): The temperature for the model (controls randomness).\n",
    "        max_tokens (int): The maximum number of tokens for the translation.\n",
    "        top_p (float): The top_p parameter for nucleus sampling (controls diversity).\n",
    "        frequency_penalty (float): The frequency penalty to reduce repetition.\n",
    "        presence_penalty (float): The presence penalty to encourage new topics.\n",
    "        echo (bool): Whether to include the prompt in the completion.\n",
    "        logit_bias (dict): Adjusts the likelihood of specified tokens appearing in the completion.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated text.\n",
    "    \"\"\"\n",
    "    prompt = f\"Translate the following text from {source_lang} to {target_lang}:\\n\\n{text}\\n\\nTranslation:\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-3.5-turbo\",  # Use the appropriate model name\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        echo=echo,\n",
    "        logit_bias=logit_bias,\n",
    "        stop=[\"\\n\"]\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Define the text to be translated\n",
    "text_to_translate = \"Hello, how are you?\"\n",
    "\n",
    "# Translate the text with default parameters\n",
    "translated_text = translate_text(text_to_translate, source_lang=\"English\", target_lang=\"Spanish\")\n",
    "\n",
    "print(\"Translated Text:\", translated_text)\n",
    "\n",
    "# Experiment with different parameters\n",
    "translated_text_deterministic = translate_text(text_to_translate, source_lang=\"English\", target_lang=\"Spanish\", temperature=0.2)\n",
    "print(\"\\nDeterministic Translation:\", translated_text_deterministic)\n",
    "\n",
    "translated_text_creative = translate_text(text_to_translate, source_lang=\"English\", target_lang=\"Spanish\", temperature=1.0, max_tokens=150, top_p=1.0, frequency_penalty=0.5, presence_penalty=0.5)\n",
    "print(\"\\nCreative and Diverse Translation:\", translated_text_creative)\n",
    "\n",
    "translated_text_with_echo = translate_text(text_to_translate, source_lang=\"English\", target_lang=\"Spanish\", echo=True)\n",
    "print(\"\\nTranslation with Echo:\", translated_text_with_echo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool that determines the sentiment of a given text (positive, negative, neutral).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key = 'your-api-key'\n",
    "\n",
    "def analyze_sentiment(text, temperature=0.7, max_tokens=60, top_p=0.9, frequency_penalty=0, presence_penalty=0, n=1, logprobs=None):\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of the given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to analyze.\n",
    "        temperature (float): The temperature for the model (controls randomness).\n",
    "        max_tokens (int): The maximum number of tokens for the response.\n",
    "        top_p (float): The top_p parameter for nucleus sampling (controls diversity).\n",
    "        frequency_penalty (float): The frequency penalty to reduce repetition.\n",
    "        presence_penalty (float): The presence penalty to encourage new topics.\n",
    "        n (int): The number of completions to generate.\n",
    "        logprobs (int): Returns the log probabilities of the top logprobs tokens.\n",
    "\n",
    "    Returns:\n",
    "        list of str: List of sentiments (positive, negative, neutral).\n",
    "    \"\"\"\n",
    "    prompt = f\"Analyze the sentiment of the following text and categorize it as positive, negative, or neutral:\\n\\n{text}\\n\\nSentiment:\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        n=n,\n",
    "        logprobs=logprobs,\n",
    "        stop=[\"\\n\"]\n",
    "    )\n",
    "    \n",
    "    sentiments = [choice.text.strip() for choice in response.choices]\n",
    "    return sentiments, response if logprobs is not None else sentiments\n",
    "\n",
    "# Define the text to analyze\n",
    "text_to_analyze = \"I am so happy with the service I received!\"\n",
    "\n",
    "# Analyze the sentiment with default parameters\n",
    "sentiments, response = analyze_sentiment(text_to_analyze, logprobs=5)\n",
    "\n",
    "print(\"Sentiments:\", sentiments)\n",
    "\n",
    "if response:\n",
    "    print(\"\\nLogprobs:\", response.choices[0].logprobs)\n",
    "\n",
    "# Experiment with different parameters\n",
    "sentiments_deterministic = analyze_sentiment(text_to_analyze, temperature=0.2)[0]\n",
    "print(\"\\nDeterministic Sentiment:\", sentiments_deterministic)\n",
    "\n",
    "sentiments_creative = analyze_sentiment(text_to_analyze, temperature=1.0, max_tokens=80, top_p=1.0, frequency_penalty=0.5, presence_penalty=0.5, n=3)[0]\n",
    "print(\"\\nCreative and Diverse Sentiments:\", sentiments_creative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Create a text completion application that generates text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key = 'your-api-key'\n",
    "\n",
    "def generate_text(prompt, temperature=0.7, max_tokens=100, top_p=0.9, frequency_penalty=0, presence_penalty=0, stop=None, best_of=1):\n",
    "    \"\"\"\n",
    "    Generates text based on the given prompt.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The initial text prompt.\n",
    "        temperature (float): The temperature for the model (controls randomness).\n",
    "        max_tokens (int): The maximum number of tokens for the completion.\n",
    "        top_p (float): The top_p parameter for nucleus sampling (controls diversity).\n",
    "        frequency_penalty (float): The frequency penalty to reduce repetition.\n",
    "        presence_penalty (float): The presence penalty to encourage new topics.\n",
    "        stop (list of str): Sequences where the model will stop generating further tokens.\n",
    "        best_of (int): Generates multiple completions server-side and returns the best one.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        stop=stop,\n",
    "        best_of=best_of\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Define the initial prompt\n",
    "initial_prompt = \"Once upon a time, in a land far, far away, there was a\"\n",
    "\n",
    "# Generate text with default parameters\n",
    "generated_text = generate_text(initial_prompt)\n",
    "\n",
    "print(\"Generated Text:\", generated_text)\n",
    "\n",
    "# Experiment with different parameters\n",
    "text_high_temp = generate_text(initial_prompt, temperature=1.0)\n",
    "print(\"\\nHigh Temperature Text:\", text_high_temp)\n",
    "\n",
    "text_low_temp = generate_text(initial_prompt, temperature=0.2)\n",
    "print(\"\\nLow Temperature Text:\", text_low_temp)\n",
    "\n",
    "text_high_max_tokens = generate_text(initial_prompt, max_tokens=200)\n",
    "print(\"\\nHigh Max Tokens Text:\", text_high_max_tokens)\n",
    "\n",
    "text_low_max_tokens = generate_text(initial_prompt, max_tokens=50)\n",
    "print(\"\\nLow Max Tokens Text:\", text_low_max_tokens)\n",
    "\n",
    "text_high_top_p = generate_text(initial_prompt, top_p=1.0)\n",
    "print(\"\\nHigh Top P Text:\", text_high_top_p)\n",
    "\n",
    "text_low_top_p = generate_text(initial_prompt, top_p=0.5)\n",
    "print(\"\\nLow Top P Text:\", text_low_top_p)\n",
    "\n",
    "text_high_freq_penalty = generate_text(initial_prompt, frequency_penalty=0.5)\n",
    "print(\"\\nHigh Frequency Penalty Text:\", text_high_freq_penalty)\n",
    "\n",
    "text_high_pres_penalty = generate_text(initial_prompt, presence_penalty=0.5)\n",
    "print(\"\\nHigh Presence Penalty Text:\", text_high_pres_penalty)\n",
    "\n",
    "text_with_stop = generate_text(initial_prompt, stop=[\"land\", \"far\"])\n",
    "print(\"\\nText with Stop Sequences:\", text_with_stop)\n",
    "\n",
    "text_best_of = generate_text(initial_prompt, best_of=3)\n",
    "print(\"\\nBest Of Text:\", text_best_of)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: Google Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a basic chatbot using Google Vertex AI to answer questions about a given topic.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Develop a script that summarizes long text inputs using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Create a tool that translates text from one language to another using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool using Google Vertex AI to determine the sentiment of a given text.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Develop a text completion application using Google Vertex AI to generate text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
